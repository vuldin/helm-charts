# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This file contains values for variables referenced from yaml files in the templates directory.
#
# For further information on Helm templating see the documentation at:
#  https://helm.sh/docs/chart_template_guide/values_files/

# Common parameters
#
# Override redpanda.name template
nameOverride: ""
# Override redpanda.fullname template
fullnameOverride: ""
# Default kuberentes cluster domain
clusterDomain: cluster.local
# Additional labels added to all Kubernetes objects
commonLabels: {}

# Redpanda parameters
#
image:
  repository: vectorized/redpanda
  # Redpanda version. This determines the installed version (not Chart.appVersion)
  tag: v22.1.4
  # The imagePullPolicy will default to Always when the tag is 'latest'
  pullPolicy: IfNotPresent
# Your license key (optional)
license_key: ""
# Your organization name (optional)
organization: ""
#
# Authentication
auth:
  #
  # SASL configuration
  sasl:
    enabled: false
    # user list
    # TODO create user at startup
    users:
      - name: admin
        # Password for the user. This will be used to generate a secret
        # password: password
        # If password isn't given, then the secretName must point to an already existing secret
        # secretName: adminPassword
  #
  # TLS configuration
  tls:
    # Enable global TLS, which turns on TLS by default for all listeners
    # Each listener must include a certificate name in its TLS section
    # Any certificates in auth.tls.certs will still be loaded if enabled is false
    # This is because listeners may enable TLS individually (see listeners.<listener name>.tls.enabled)
    enabled: false
    # list all certificates below, then reference a certificate's name in each listener (see listeners.<listener name>.tls.cert)
    certs:
      # This is the certificate name that is used to associate the certificate with a listener
      # See listeners.<listener group>.tls.cert for more information
      kafka:
        # issuerRef:
        #   name: redpanda-cert1-selfsigned-issuer
        # The caEnabled flag determines whether the ca.crt file is included in the TLS mount path on each Redpanda pod
        caEnabled: true
        # duration: 43800h
      rpc:
        caEnabled: true
      admin:
        caEnabled: true
      proxy:
        caEnabled: true
      schema:
        caEnabled: true
#
# Listeners
listeners:
  # Admin API listener
  admin:
    enabled: true
    address: 0.0.0.0
    # The port for the admin server
    # Metrics are served off of this port at /metrics
    port: 9644
    external:
      enabled: false
    tls:
      # Optional flag to override the global TLS enabled flag
      # enabled: true
      # Name of the certificate used for TLS. Enable TLS with auth.tls.enabled
      # Must match a cert registered at auth.tls.certs
      cert: admin
      # If true, the truststore file for this listener will be included in the ConfigMap
      requireClientAuth: false
  # Kafka API listeners
  # The kafka listener group cannot be disabled
  kafka:
    # The first entry is used to set up the headless service.
    # Kafka external and internal advertised listeners will be
    # derived from the configuration. External advertised listener will be via
    # node port unless load balancer is configured.
    endpoints:
    # TODO POD_IP doesn't mean anything to the user
    # maybe make address optional, and if not set then default is $(POD_IP)
    - address: $(POD_IP)
      name: default
      port: 9092
      external:
        enabled: false
        subdomain: "helm-test.redpanda.com"
      tls:
        # enabled: true
        cert: kafka
        requireClientAuth: false
  # REST API listeners (aka PandaProxy)
  rest:
    enabled: true
    # PandaProxy is a kafka client that connects to an endpoint from listeners.kafka.endpoints
    kafkaEndpoint: default
    endpoints:
    - address: 0.0.0.0
      name: default
      port: 8082
      external:
        enabled: true
      tls:
        # enabled: true
        cert: proxy
        requireClientAuth: false
  # RPC listener
  # The RPC listener cannot be disabled
  rpc:
    address: 0.0.0.0
    port: 33145
    external:
      enabled: false
    tls:
      # enabled: true
      cert: rpc
      requireClientAuth: false
  # Schema registry listeners
  schemaRegistry:
    enabled: true
    # Schema Registry is a kafka client that connects to an endpoint from listeners.kafka.endpoints
    kafkaEndpoint: default
    endpoints:
    - address: 0.0.0.0
      name: default
      port: 8081
      external:
        enabled: true
      tls:
        # enabled: true
        cert: schema
        requireClientAuth: false
#
# Log level
# Valid values are info, warn, debug, trace
logLevel: info
#
# Persistence
# TODO modify to conform to https://github.com/bitnami/charts/blob/master/bitnami/kafka/values.yaml#L888
storage:
  # TODO make use of enabled parameter
  enabled: true
  # Absolute path on host to store Redpanda's data.
  # If not specified, then `emptyDir` will be used instead.
  # If specified, but `persistentVolume.enabled` is `true`, then has no effect.
  hostPath: ""
  # If `enabled` is `true` then a PersistentVolumeClaim will be created and
  # used to store Redpanda's data, otherwise `hostPath` is used.
  persistentVolume:
    # TODO possibly replace with storage.enabled
    enabled: true
    # TODO lower default size to a more reasonable size (10 - 50 Mi)
    size: 100Gi
    # If defined, then `storageClassName: <storageClass>`.
    # If set to "-", then `storageClassName: ""`, which disables dynamic
    # provisioning.
    # If undefined or empty (default), then no `storageClassName` spec is set,
    # so the default provisioner will be chosen (gp2 on AWS, standard on
    # GKE, AWS & OpenStack).
    storageClass: ""
    # Additional labels to apply to the created PersistentVolumeClaims.
    labels: {}
    # Additional annotations to apply to the created PersistentVolumeClaims.
    annotations: {}

# Redpanda Keeper configuration
rpk:
  # Enables memory locking.
  enable_memory_locking: false
  # Send usage stats back to vectorized
  enable_usage_stats: true
  #
  # This should be set to true unless with have CPU affinity
  # https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy
  # Equivalent to passing the following to redpanda
  # --idle-poll-time-us 0 --thread-affinity 0 --poll-aio 0
  overprovisioned: true
  # Increases the number of allowed asynchronous IO events
  tune_aio_events: false
  # Syncs NTP
  tune_clocksource: false
  # Installs a custom script to process coredumps and save them to the given directory.
  tune_coredump: false
  #
  # Disables hyper-threading, sets the ACPI-cpufreq governor to 'performance'.
  # Also if system reboot is allowed it disables: Intel P-States, Intel C-States, and Turbo Boost
  # Setting to true is not possible while running in a container.
  tune_cpu: false
  #
  # Distributes IRQs across cores with the method deemed the most appropriate for the
  # current device type (i.e. NVMe)
  # Setting to true is not possible while running in a container.
  tune_disk_irq: false

statefulset:
  replicas: 3
  updateStrategy:
    type: OrderedReady 
  podManagementPolicy: Parallel
  budget:
    maxUnavailable: 1
  # Additional annotations to apply to the Pods of this StatefulSet.
  annotations: {}
  # Redpanda makes use of a thread per core model which is described here:
  # https://vectorized.io/blog/tpc-buffers/ For this reason Redpanda should
  # only be given full cores for requests and limits. The recommendation
  # for memory for Redpanda is at least 2GB per core. These values will also
  # affect the --smp and --memory flags which are passed to Redpanda.
  #
  # Limits are only specified as to provide Guaranteed QoS:
  # https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed
  #
  # To improve performance further it is recommended to enable CPU Affinity:
  # https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy
  #
  # NOTE: You can increase the number of cores but decreasing the number
  # is not supported currently.
  # https://github.com/vectorizedio/redpanda/issues/350#
  resources:
    limits:
      cpu: "1"
      memory: 2.5Gi
  # Redpanda memory parameter can not exceed POD resource memory limit
  # Example calculation:
  # statefulset.redpanda.memory = 0.8 statefulset.resources.limits.memory
  #
  # NOTE: Memory flag does not support float numbers and support only E, P, T, G, M, k suffixes
  # REF:
  # https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory
  # https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/
  redpanda:
    memory: 2G
    reservedMemory: 400k
  # Inter-Pod Affinity rules for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  podAffinity: {}
  # Anti-affinity rules for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # You may either toggle options below for default anti-affinity rules,
  # or specify the whole set of anti-affinity rules instead of them.
  podAntiAffinity:
    # The topologyKey to be used.
    # Can be used to spread across different nodes, AZs, regions etc.
    topologyKey: kubernetes.io/hostname
    # Type of anti-affinity rules: either `soft`, `hard` or empty value (which
    # disables anti-affinity rules).
    type: soft
    # Weight for `soft` anti-affinity rules.
    # Does not apply for other anti-affinity types.
    weight: 100
  # Node selection constraints for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}
  # PriorityClassName given to Pods of this StatefulSet
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""
  # Taints to be tolerated by Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  # https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
  # When using persistent storage the volume will be mounted as root. In order for redpanda to use the volume
  # we must set the fsGroup to the uid of redpanda, which is 101
  podSecurityContext:
    fsGroup: 101
    # runAsNonRoot: true
    # runAsUser: 1000

# Service account management
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# This section allows modifying various Redpanda settings not covered in other sections above.
# These values do not pertain to the kubernetes objects created with helm.
# Instead these parameters get passed directly to the Redpanda binary at startup.
# See https://docs.redpanda.com/docs/cluster-administration/configuration/
# There are three subsections, cluster, tunable and node. The cluster and tunable sections
# are written to the cluster .bootstrap.yaml and read into central config upon startup.
# Values in the node section are written to redpanda.yaml.
config:
  cluster:
    # auto_create_topics_enabled: true                             # Allow topic auto creation
    # transaction_coordinator_replication: 1                       # Replication factor for a transaction coordinator topic
    # id_allocator_replication: 1                                  # Replication factor for an ID allocator topic
    # disable_metrics: false                                       # Disable registering metrics
    # enable_coproc: false                                         # Enable coprocessing mode
    # enable_idempotence: false                                    # Enable idempotent producer
    # enable_pid_file: true                                        # Enable pid file; You probably don't want to change this
    # enable_transactions: false                                   # Enable transactions
    # group_max_session_timeout_ms: 300s                           # The maximum allowed session timeout for registered consumers; Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures; Default quota tracking window size in milliseconds
    # group_min_session_timeout_ms: Optional                       # The minimum allowed session timeout for registered consumers; Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating
    # kafka_group_recovery_timeout_ms: 30000ms                     # Kafka group recovery timeout expressed in milliseconds
    # kafka_qdc_enable: false                                      # Enable kafka queue depth control
    # kafka_qdc_max_latency_ms: 80ms                               # Max latency threshold for kafka queue depth control depth tracking
    # log_cleanup_policy: deletion                                 # Default topic cleanup policy
    # log_compaction_interval_ms: 5min                             # How often do we trigger background compaction
    # log_compression_type: producer                               # Default topic compression type
    # log_message_timestamp_type: create_time                      # Default topic messages timestamp type
    # retention_bytes: None                                        # max bytes per partition on disk before triggering a compaction
    # rm_sync_timeout_ms: 2000ms 
    # rm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the partition level
    # target_quota_byte_rate: 2GB                                  # Target quota byte rate in bytes per second
    # tm_sync_timeout_ms: 2000ms                                   # Time to wait state catch up before rejecting a request
    # tm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the transaction coordinator level
    # transactional_id_expiration_ms: 10080min                     # Producer ids are expired once this time has elapsed after the last write with the given producer ID
  tunable:
    # alter_topic_cfg_timeout_ms: 5s                              # Time to wait for entries replication in controller log when executing alter configuration request
    # compacted_log_segment_size: 256MiB                           # How large in bytes should each compacted log segment be (default 256MiB)
    # controller_backend_housekeeping_interval_ms: 1s              # Interval between iterations of controller backend housekeeping loop
    # coproc_max_batch_size: 32kb                                  # Maximum amount of bytes to read from one topic read
    # coproc_max_inflight_bytes: 10MB                              # Maximum amountt of inflight bytes when sending data to wasm engine
    # coproc_max_ingest_bytes: 640kb                               # Maximum amount of data to hold from input logs in memory
    # coproc_offset_flush_interval_ms: 300000ms                    # Interval for which all coprocessor offsets are flushed to disk
    # create_topic_timeout_ms: 2000ms                              # Timeout (ms) to wait for new topic creation
    # default_num_windows: 10                                      # Default number of quota tracking windows
    # default_window_sec: 1000ms                                   # Default quota tracking window size in milliseconds
    # delete_retention_ms: 10080min                                # delete segments older than this (default 1 week)
    # developer_mode: optional                                     # Skips most of the checks performed at startup
    # disable_batch_cache: false                                   # Disable batch cache in log manager
    # fetch_reads_debounce_timeout: 1ms                            # Time to wait for next read in fetch request when requested min bytes wasn't reached
    # fetch_session_eviction_timeout_ms: 60s                       # Minimum time before which unused session will get evicted from sessions; Maximum time after which inactive session will be deleted is two time given configuration valuecache
    # group_initial_rebalance_delay: 300ms                         # Extra delay (ms) added to rebalance phase to wait for new members
    # group_new_member_join_timeout: 30000ms                       # Timeout for new member joins
    # group_topic_partitions: 1                                    # Number of partitions in the internal group membership topic
    # id_allocator_batch_size: 1000                                # ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted
    # id_allocator_log_capacity: 100                               # Capacity of the id_allocator log in number of messages; Once it reached id_allocator_stm should compact the log
    # join_retry_timeout_ms: 5s                                    # Time between cluster join retries in milliseconds
    # kafka_qdc_idle_depth: 10                                     # Queue depth when idleness is detected in kafka queue depth control
    # kafka_qdc_latency_alpha: 0.002                               # Smoothing parameter for kafka queue depth control latency tracking
    # kafka_qdc_max_depth: 100                                     # Maximum queue depth used in kafka queue depth control
    # kafka_qdc_min_depth: 1                                       # Minimum queue depth used in kafka queue depth control
    # kafka_qdc_window_count: 12                                   # Number of windows used in kafka queue depth control latency tracking
    # kafka_qdc_window_size_ms: 1500ms                             # Window size for kafka queue depth control latency tracking
    # kvstore_flush_interval: 10ms                                 # Key-value store flush interval (ms)
    # kvstore_max_segment_size: 16MB                               # Key-value maximum segment size (bytes)
    # log_segment_size: 1GB                                        # How large in bytes should each log segment be (default 1G)
    # max_compacted_log_segment_size: 5GB                          # Max compacted segment size after consolidation
    # max_kafka_throttle_delay_ms: 60000ms                         # Fail-safe maximum throttle delay on kafka requests
    # metadata_dissemination_interval_ms: 3000ms                   # Interaval for metadata dissemination batching
    # metadata_dissemination_retries: 10                           # Number of attempts of looking up a topic's meta data like shard before failing a request
    # metadata_dissemination_retry_delay_ms: 500ms                 # Delay before retry a topic lookup in a shard or other meta tables
    # quota_manager_gc_sec: 30000ms                                # Quota manager GC frequency in milliseconds
    # raft_learner_recovery_rate: 104857600                          # Raft learner recovery rate in bytes per second
    # raft_heartbeat_disconnect_failures: 3 #After how many failed heartbeats to forcibly close an unresponsive TCP connection. Set to 0 to disable force disconnection.
    # raft_heartbeat_interval_ms: 150 #The interval in ms between raft leader heartbeats.
    # raft_heartbeat_timeout_ms: 3000	#Raft heartbeat RPC timeout.
    # raft_io_timeout_ms: 10000	#Raft I/O timeout.
    # raft_max_concurrent_append_requests_per_follower: 16	#Maximum number of concurrent append entries requests sent by leader to one follower.
    # raft_max_recovery_memory: 33554432	#Maximum memory that can be used for reads in the raft recovery process.
    # raft_recovery_default_read_size: 524288	#Default size of read issued during raft follower recovery.
    # raft_replicate_batch_window_size: 1048576	#Maximum size of requests cached for replication.
    # raft_smp_max_non_local_requests:   #Maximum number of x-core requests pending in Raft seastar::smp group. (for more details look at seastar::smp_service_group documentation).
    # raft_timeout_now_timeout_ms: 1000   #Timeout for a timeout now request.
    # raft_transfer_leader_recovery_timeout_ms: 1000	#Timeout waiting for follower recovery when transferring leadership.
    # raft_election_timeout_ms: 1500ms                             # Election timeout expressed in milliseconds TBD - election_time_out
    # readers_cache_eviction_timeout_ms: 30s                       # Duration after which inactive readers will be evicted from cache
    # reclaim_growth_window: 3000ms                                # Length of time in which reclaim sizes grow
    # reclaim_max_size: 4MB                                        # Maximum batch cache reclaim size
    # reclaim_min_size: 128KB                                      # Minimum batch cache reclaim size
    # reclaim_stable_window: 10000ms                               # Length of time above which growth is reset
    # recovery_append_timeout_ms: 5s                               # Timeout for append entries requests issued while updating stale follower
    # release_cache_on_segment_roll: false                         # Free cache when segments roll
    # replicate_append_timeout_ms: 3s                              # Timeout for append entries requests issued while replicating entries
    # segment_appender_flush_timeout_ms: 1ms                       # Maximum delay until buffered data is written
    # wait_for_leader_timeout_ms: 5000ms                           # Timeout (ms) to wait for leadership in metadata cache
  node:
    # node_id:                                                     # Unique ID identifying a node in the cluster
    # data_directory:                                              # Place where redpanda will keep the data
    # admin_api_doc_dir: /usr/share/redpanda/admin-api-doc         # Admin API doc directory
    # api_doc_dir: /usr/share/redpanda/proxy-api-doc               # API doc directory
    # coproc_supervisor_server: 127.0.0.1:43189                    # IpAddress and port for supervisor service
    # dashboard_dir: None                                          # serve http dashboard on / url
    # rack: None                                                   # Rack identifier

  # Invalid properties
  # Any of these properties will be ignored. These otherwise valid properties are not allowed
  # to be used in this section since they impact deploying Redpanda in Kubernetes.
  # Make use of the above sections to modify these values instead (see comments below).
  # admin: 127.0.0.1:9644                               # Address and port of admin server
  # admin_api_tls: validate_many                                # TLS configuration for admin HTTP server
  # advertised_kafka_api: None                                # Address of Kafka API published to the clients
  # advertised_pandaproxy_api: None                               # Rest API address and port to publish to client
  # advertised_rpc_api: None                                # Address of RPC endpoint published to other cluster members
  # cloud_storage_access_key: None                                # AWS access key
  # cloud_storage_api_endpoint: None                                # Optional API endpoint
  # cloud_storage_api_endpoint_port: 443                                # TLS port override
  # cloud_storage_bucket: None                                # AWS bucket that should be used to store data
  # cloud_storage_disable_tls: false                               # Disable TLS for all S3 connections
  # cloud_storage_enabled: false                                # Enable archival storage
  # cloud_storage_max_connections: 20                                # Max number of simultaneous uploads to S3
  # cloud_storage_reconciliation_ms: 10s                                # Interval at which the archival service runs reconciliation (ms)
  # cloud_storage_region: None                                # AWS region that houses the bucket used for storage
  # cloud_storage_secret_key: None                                # AWS secret key
  # cloud_storage_trust_file: None                                # Path to certificate that should be used to validate server certificate during TLS handshake
  # default_topic_partitions: 1                                # Default number of partitions per topic
  # default_topic_replications: 3                                # Default replication factor for new topics
  # enable_admin_api                                Enable the admin API                                true
  # enable_sasl                                Enable SASL authentication for Kafka connections                                false
  # kafka_api                                Address and port of an interface to listen for Kafka API requests                                127.0.0.1:9092
  # kafka_api_tls                                TLS configuration for Kafka API endpoint                                None
  # pandaproxy_api                                Rest API listen address and port                                0.0.0.0:8082
  # pandaproxy_api_tls                                TLS configuration for Pandaproxy api                                validate_many
  # rpc_server                                IP address and port for RPC server                                127.0.0.1:33145
  # rpc_server_tls                                TLS configuration for RPC server                                validate
  # seed_servers                                List of the seed servers used to join current cluster; If the seed_server list is empty the node will be a cluster root and it will form a new cluster                                None
  # superusers                                List of superuser usernames                                None
